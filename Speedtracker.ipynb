{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run this cell to set up\n",
    "#\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import os\n",
    "from os.path import join, abspath, dirname, exists\n",
    "\n",
    "base_url = 'https://andresvidal.github.io/speedtracker-jet'\n",
    "data_path = join(os.getcwd(), 'wpt_data')\n",
    "temp_path = join(os.getcwd(), 'wpt_temp')\n",
    "\n",
    "#### GET DATA ################################\n",
    "\n",
    "def get_speedtracker_tests(profile_slug):\n",
    "    data = requests.get(f'{base_url}/profiles.json').json()\n",
    "    st_tests = {}\n",
    "    for i, profile in enumerate(data):\n",
    "        st_tests[profile['slug']] = []\n",
    "        for test in profile['tests']:\n",
    "            date = str(test)\n",
    "            r_json = f\"{base_url}/results/{profile['slug']}/{date[:4]}/{date[-2:]}.json\"\n",
    "            st_tests[profile['slug']].append(r_json)\n",
    "    return st_tests[profile_slug]\n",
    "\n",
    "def get_speedtracker_wpt_ids(speedtracker_tests):\n",
    "    wpt_ids = []\n",
    "    for test in speedtracker_tests:\n",
    "        t = requests.get(test)\n",
    "        data = t.json()\n",
    "        wpt_ids = wpt_ids + data['_r']['id']\n",
    "    return wpt_ids\n",
    "\n",
    "def get_missing_wpt_tests(missing_tests_csv, slug=None):\n",
    "    '''Returns tuple (DataFrame, list) of missing tests by slug'''\n",
    "    try:\n",
    "        mdf = pd.read_csv(missing_tests_csv, index_col=None)\n",
    "        return (mdf, mdf['wpt_id'].unique().tolist())\n",
    "    except:\n",
    "        return (pd.DataFrame(), [])\n",
    "\n",
    "def get_wpt_tests(wpt_ids, slug='test', save=True):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d-%H%M\")\n",
    "    \n",
    "    csv_file = join(data_path, f\"{slug}.csv\")\n",
    "    missing_tests_csv = join(data_path, f\"missing_wpt_tests.csv\")\n",
    "    \n",
    "    print(f\"{ts} Getting tests for {slug} profile...\")\n",
    "    \n",
    "    # returns mdf DataFrame, a list of ids\n",
    "    mdf, missing_wpt_ids = get_missing_wpt_tests(missing_tests_csv)\n",
    "    \n",
    "    missing_tests = 0\n",
    "    new_tests = 0\n",
    "    \n",
    "    # Load csv if exists\n",
    "    try:\n",
    "        wpt_df = pd.read_csv(csv_file, index_col=None)\n",
    "        tests_exist = wpt_df['data.id'].tolist() + missing_wpt_ids\n",
    "        collection = [wpt_df]\n",
    "    except FileNotFoundError:\n",
    "        tests_exist = [] + missing_wpt_ids\n",
    "        collection = []\n",
    "    \n",
    "    for test in wpt_ids:\n",
    "        if test not in tests_exist:\n",
    "            wpt = requests.get(f'https://www.webpagetest.org/result/{test}/?f=json').json()\n",
    "            try:\n",
    "                # del large data keys\n",
    "                delkeys = ['median', 'lighthouse', 'runs', 'standardDeviation']\n",
    "                for key in delkeys:\n",
    "                    if key in wpt['data'].keys():\n",
    "                        del wpt['data'][key]\n",
    "\n",
    "                ts = wpt['data']['completed']\n",
    "                wpt['speedtracker.slug'] = slug\n",
    "                wpt['speedtracker.year'] = datetime.fromtimestamp(ts).strftime('%Y')\n",
    "                wpt['speedtracker.month'] = datetime.fromtimestamp(ts).strftime('%m')\n",
    "                wpt['speedtracker.day'] = datetime.fromtimestamp(ts).strftime('%d')\n",
    "                wpt['speedtracker.week'] = datetime.fromtimestamp(ts).isocalendar()[1]\n",
    "\n",
    "                # flatten JSON\n",
    "                collection.append( json_normalize(wpt) )\n",
    "                \n",
    "                new_tests += 1\n",
    "            except Exception as e:\n",
    "                statusText = wpt.get('statusText', e)\n",
    "                if 'Test not found' in statusText:\n",
    "                    missing_tests += 1\n",
    "                    mdf = mdf.append( pd.DataFrame([[test, slug, ts]], \n",
    "                                                   columns=['wpt_id', 'speedtracker.slug', 'timestamp']) )\n",
    "                    #print('get_wpt_tests exception:', statusText, test, slug)\n",
    "\n",
    "    wpt_df = pd.concat(collection).drop_duplicates(subset='data.id', inplace=False).reset_index(drop=True)\n",
    "    \n",
    "    if save:\n",
    "        if not exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "            print(\"{ts} Created Directory \" , data_path)\n",
    "        \n",
    "        if new_tests > 0:\n",
    "            print(f\"{ts} Saving: {csv_file}\")\n",
    "            wpt_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        if missing_tests > 0:\n",
    "            print(f\"{ts} Saving: {missing_tests_csv}\")\n",
    "            mdf.to_csv(missing_tests_csv, index=False)\n",
    "        \n",
    "    print(f\"{ts} Done with {slug}. WPT is missing {missing_tests} tests.\\n\")\n",
    "    return wpt_df\n",
    "\n",
    "\n",
    "def get_wpt_from_speedtracker_tests(profile_slug, save=True):\n",
    "    speedtracker_tests = get_speedtracker_tests(profile_slug)\n",
    "    wpt_ids = get_speedtracker_wpt_ids(speedtracker_tests)\n",
    "    get_wpt_tests(wpt_ids, slug=profile_slug, save=save)\n",
    "    \n",
    "#### PARSE DATA ################################\n",
    "\n",
    "def get_pagetype(profile_slug):\n",
    "    '''Pagetype regex for Jet profiles only. like: browse22 = browse'''\n",
    "    pagetype = re.search(r'([a-z]+)', profile_slug)\n",
    "    if pagetype:\n",
    "        pagetype = pagetype.group(0)\n",
    "    return pagetype\n",
    "\n",
    "def compare(before, after, profile_slug='', fromdates='', todates=''):\n",
    "    data = {}\n",
    "    for item in after:\n",
    "        data[item] = {}\n",
    "        data[item][f'{profile_slug} {fromdates} (before - avg)'] = before[item]\n",
    "        data[item][f'{profile_slug} {todates} (after - avg)'] = after[item]\n",
    "        #data[item][f'{profile_slug} delta (seconds)'] = after[item] - before[item]\n",
    "        data[item][f'{profile_slug} delta %'] = round(((after[item] - before[item])/before[item])*100, 2)\n",
    "    return data\n",
    "\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    \n",
    "    # Carefull... returns full df but filters out outlier rows only\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out\n",
    "\n",
    "def validate_outliers(df, col_name, start_date=None, end_date=None):\n",
    "    if start_date and end_date:\n",
    "        df['date'] = pd.to_datetime(df['data.completed'], unit='s') \n",
    "        mask = (df['date'] > parse(start_date)) & (df['date'] <= parse(end_date))\n",
    "        df = df[mask]\n",
    "\n",
    "    sns.boxplot(df[col_name])\n",
    "\n",
    "def mean(profile_slug, start_date, end_date, counts=False):\n",
    "    csv_file = join(data_path, f\"{profile_slug}.csv\")\n",
    "    \n",
    "    # Choose metrics to include in average calculations\n",
    "    metrics  = ['data.average.firstView.TTFB', \n",
    "                \n",
    "                'data.average.firstView.firstPaint',\n",
    "                #'data.average.firstView.PerformancePaintTiming.first-paint',\n",
    "                \n",
    "                'data.average.firstView.firstContentfulPaint',\n",
    "                #'data.average.firstView.PerformancePaintTiming.first-contentful-paint',\n",
    "                #'data.average.firstView.chromeUserTiming.firstContentfulPaint',\n",
    "                \n",
    "                'data.average.firstView.firstMeaningfulPaint',\n",
    "                \n",
    "                'data.average.firstView.FirstInteractive',\n",
    "                'data.average.firstView.TimeToInteractive',\n",
    "                'data.average.firstView.domInteractive',\n",
    "                #'data.average.firstView.LastInteractive',\n",
    "                \n",
    "                'data.average.firstView.SpeedIndex',\n",
    "                'data.average.firstView.fullyLoaded',\n",
    "                ]\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['date'] = pd.to_datetime(df['data.completed'], unit='s') \n",
    "    \n",
    "    #adding 1 day assumes comparison with top of the previous day. Note: exact end_date with hours will not work.\n",
    "    mask = (df['date'] > parse(start_date)) & (df['date'] < parse(end_date)+relativedelta(days=+1))\n",
    "    df = df[mask]\n",
    "    \n",
    "    dfo = {}\n",
    "    for metric in metrics:\n",
    "        # remove outliers and get mean per metric.\n",
    "        dfo[metric] = round(remove_outlier(df, metric)[metric].mean() / 1000, 3) # seconds\n",
    "        #print(f'{profile_slug} {metric} {dfo[metric]}')\n",
    "        if counts:\n",
    "            dfo[f'{metric}_count'] = df[metric].count()\n",
    "    \n",
    "    return dfo\n",
    "    #return df\n",
    "    \n",
    "def run_compare(profile_slugs, from_start, from_end, to_start, to_end):\n",
    "    collection = []\n",
    "    from_date_range = f\"{from_start}_{from_end}\"\n",
    "    to_date_range = f\"{to_start}_{to_end}\"\n",
    "    \n",
    "    if type(profile_slugs) == str:\n",
    "        profile_slugs = [profile_slugs]\n",
    "    \n",
    "    for profile_slug in profile_slugs:\n",
    "        before = mean(profile_slug, from_start, from_end)\n",
    "        after = mean(profile_slug, to_start, to_end)\n",
    "\n",
    "        # times will be in seconds\n",
    "        results = compare(before, after, profile_slug, from_date_range, to_date_range)\n",
    "        collection.append(pd.DataFrame( results ) )\n",
    "\n",
    "    return pd.concat(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run this cell to download the following profile data from webpagetest as csv files\n",
    "# See first cell to update: data_path = path\n",
    "# Run first cell again if modified\n",
    "#\n",
    "\n",
    "speedtracker_profiles_download = [\n",
    "    'home',\n",
    "    'home-noanalytics',\n",
    "    'product22',\n",
    "    'product23',\n",
    "    'product23-v5',\n",
    "    'product2',\n",
    "    'product2-noanalytics',\n",
    "    'product24',\n",
    "    'product25',\n",
    "    'browse22',\n",
    "    'browse24',\n",
    "    'browse26',\n",
    "    'browse27',\n",
    "    'search',\n",
    "    'search23',\n",
    "    'search26',\n",
    "\n",
    "    'walmart-home',\n",
    "    'target-home',\n",
    "    'nike-home',\n",
    "    'amazon-home',\n",
    "    'bhphoto-home',\n",
    "    'bhphoto-browse',\n",
    "    'bhphoto-product',\n",
    "    'bhphoto-search',\n",
    "    'airbnb-home',\n",
    "    'airbnb-homes',\n",
    "    'flipkart-home',\n",
    "    'freshdirect-home',\n",
    "    'freshdirect-browse',\n",
    "    'freshdirect-product',\n",
    "    'freshdirect-search',\n",
    "    'apple-home']\n",
    "\n",
    "for profile in speedtracker_profiles_download:\n",
    "    get_wpt_from_speedtracker_tests(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Define profiles to analyze\n",
    "# \n",
    "\n",
    "speedtracker_profiles = [\n",
    "    'home',\n",
    "    'product22',\n",
    "    'product2',\n",
    "    'product24',\n",
    "    'product25',\n",
    "    'browse22',\n",
    "    'browse24',\n",
    "    'browse26',\n",
    "    'browse27',\n",
    "    'search',\n",
    "    'search23',\n",
    "    'search26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use 2 weeks or more when comparing in order to avoid overpowering outliers within small samples\n",
    "#\n",
    "#\n",
    "# Pull data for the profiles defined above and save to csv\n",
    "# run_compare(profile_slugs, from_start, from_end, to_start, to_end)\n",
    "# \n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "#\n",
    "# Pre-determined run dates below based on batman sprint cycles \n",
    "#\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/01/13', '2019/01/26', '2019/01/27', '2019/02/09') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/01/27', '2019/02/09', '2019/02/10', '2019/02/16') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/01/27', '2019/02/09', '2019/02/10', '2019/02/23') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/02/10', '2019/02/23', '2019/02/24', '2019/03/02') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/02/10', '2019/02/23', '2019/02/24', '2019/03/09') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/02/24', '2019/03/09', '2019/03/10', '2019/03/16') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/02/24', '2019/03/09', '2019/03/10', '2019/03/23') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/10', '2019/03/23', '2019/03/24', '2019/03/30') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/10', '2019/03/23', '2019/03/24', '2019/04/06') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/24', '2019/04/06', '2019/04/07', '2019/04/13') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/24', '2019/04/06', '2019/04/07', '2019/04/20') # 2/2 weeks\n",
    "\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/24', '2019/04/06', '2019/04/07', '2019/04/13') # 2/1 weeks\n",
    "#df = run_compare(speedtracker_profiles, '2019/03/24', '2019/04/06', '2019/04/07', '2019/04/20') # 2/2 weeks\n",
    "\n",
    "\n",
    "# test \n",
    "# eg: tue-mon\n",
    "df = run_compare(speedtracker_profiles, '2019/01/15', '2019/01/21', '2019/04/02', '2019/04/08')\n",
    "\n",
    "# Save\n",
    "df.to_csv(join(temp_path, f'speedtracker-analysis_{ts}.csv'))\n",
    "\n",
    "# Show data table. Comment to ignore showing.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run to calculate and save Percentage Changes for profiles defined above\n",
    "#\n",
    "\n",
    "df_p = df[df.index.str.contains('%')].sort_values(by=['data.average.firstView.FirstInteractive', 'data.average.firstView.firstMeaningfulPaint'])\n",
    "df_p.to_csv(join(temp_path, f'speedtracker-analysis_pct_chng_{ts}.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run to Validate Outliers for given dates and metric (use full name with dot-notation)\n",
    "# Uses defined profiles from previous cell\n",
    "#\n",
    "# Update dates and metrics below before running\n",
    "# Note: you can uncomment/comment out removing outliers below...\n",
    "#\n",
    "\n",
    "start_date = '2019/02/24'\n",
    "end_date = '2019/03/02'\n",
    "metric = 'data.average.firstView.firstPaint'\n",
    "rmv_outliers = True\n",
    "\n",
    "boxplot_collection = []\n",
    "\n",
    "for profile_slug in speedtracker_profiles:\n",
    "    _df = pd.read_csv(join(data_path, f'{profile_slug}.csv'))\n",
    "    _df['date'] = pd.to_datetime(_df['data.completed'], unit='s')\n",
    "    _mask = (_df['date'] > parse(start_date)) & (_df['date'] < parse(end_date)+relativedelta(days=+1))\n",
    "    _dfmasked = _df[_mask]\n",
    "    \n",
    "    if rmv_outliers:\n",
    "        _dfmasked = remove_outlier(_dfmasked, metric)\n",
    "    \n",
    "    boxplot_collection.append(_dfmasked)\n",
    "\n",
    "print(f\"Date Range: {start_date} to {end_date} \\nRemove Outliers: {rmv_outliers}\")\n",
    "boxplot_df = pd.concat(boxplot_collection)\n",
    "sns.set(rc={'figure.figsize':(15,8.27)})\n",
    "sns.boxplot(x='speedtracker.slug', y=metric,width=.7, data=boxplot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.read_csv(join(data_path, 'home.csv'))\n",
    "dfc = dfc[['data.average.firstView.FirstInteractive', \n",
    "           'data.average.firstView.TimeToInteractive',\n",
    "           'data.average.firstView.firstMeaningfulPaint',\n",
    "           'data.average.firstView.SpeedIndex',\n",
    "           'data.average.firstView.render',\n",
    "           'data.average.firstView.score_compress',\n",
    "           'data.average.firstView.firstLayout',\n",
    "           'data.average.firstView.domElements',\n",
    "           'data.average.firstView.image_savings',\n",
    "           'data.average.firstView.requestsFull',\n",
    "           'data.average.firstView.score_gzip',\n",
    "           'data.average.firstView.fullyLoadedCPUpct',\n",
    "           'data.average.firstView.image_total',\n",
    "           'data.average.firstView.connections',\n",
    "           'data.average.firstView.TTFB']].dropna(axis=0)\n",
    "\n",
    "corr2 = dfc.rename(dict(zip(\n",
    "        list(dfc.columns), \n",
    "        list(dfc.columns.str.split('.').map(lambda r: r[-1])) \n",
    "    )), axis=1).corr(method='pearson', min_periods=100)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask2 = np.zeros_like(corr2, dtype=np.bool)\n",
    "mask2[np.triu_indices_from(mask2)] = True\n",
    "\n",
    "sns.heatmap(corr2, mask=mask2, cmap=cmap, center=0, robust=1,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(join(data_path, 'home.csv')).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
